{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PyFlyt.gym_envs import FlattenWaypointEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.vec_normalize import VecNormalize\n",
    "import gymnasium as gym\n",
    "\n",
    "DEFAULT_HIDDEN_UNITS = 64\n",
    "\n",
    "def tensor(x, device=\"cpu\"):\n",
    "    if torch.is_tensor(x):\n",
    "        return x\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    x = torch.tensor(x, device=torch.device(device), dtype=torch.float32)\n",
    "    return x\n",
    "\n",
    "def make_env(environment_id, log_dir):\n",
    "    def _thunk():\n",
    "        if environment_id==\"QuadX-Hover-v1\":\n",
    "            env = gym.make(f\"PyFlyt/{environment_id}\")\n",
    "        elif environment_id==\"QuadX-Wapoints-1\":\n",
    "            env = gym.make(f\"PyFlyt/{environment_id}\")\n",
    "            env = FlattenWaypointEnv(env, context_lenght=1)\n",
    "        else:\n",
    "            raise \"Uncompatible environment\"\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        env = Monitor(env, log_dir)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "        m.weight.data.mul_(1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class DACNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, num_options, device=\"cuda:0\"):\n",
    "        super(DACNetwork, self).__init__()\n",
    "\n",
    "        self.higher_net = MasterNetwork(obs_dim, num_options)\n",
    "        self.lower_nets = nn.ModuleList([LowerNetwork(obs_dim, action_dim) for _ in range(num_options)])\n",
    "        if not torch.cuda.is_available():\n",
    "            device = \"cpu\"\n",
    "        print(\"using device: %s\" % device)\n",
    "        self.device = device\n",
    "        self.apply(init_weights)\n",
    "        self.to(torch.device(self.device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = tensor(x, self.device)\n",
    "        mean = []\n",
    "        std = []\n",
    "        beta = []\n",
    "        for lower_net in self.lower_nets:\n",
    "            option_pred = lower_net(x)\n",
    "            mean.append(option_pred[\"mean_action\"].unsqueeze(1))\n",
    "            std.append(option_pred[\"std_action\"].unsqueeze(1))\n",
    "            beta.append(option_pred[\"termination_prob\"])\n",
    "        mean = torch.cat(mean, dim=1)\n",
    "        std = torch.cat(std, dim=1)\n",
    "        beta = torch.cat(beta, dim=1)\n",
    "\n",
    "        master_pred = self.higher_net(x)\n",
    "\n",
    "        return {\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"beta\": beta,\n",
    "            \"q_option\": master_pred[\"q_option\"],\n",
    "            \"master_policy\": master_pred[\"master_policy\"],\n",
    "        }\n",
    "\n",
    "\n",
    "class MasterNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, num_options):\n",
    "        super(MasterNetwork, self).__init__()\n",
    "\n",
    "        self.master_policy_net = FCNetwork(obs_dim, num_options, lambda: nn.Softmax(dim=-1))\n",
    "        self.value_net = FCNetwork(obs_dim, num_options)\n",
    "\n",
    "    def forward(self, x):\n",
    "        master_policy = self.master_policy_net(x)\n",
    "        q_option = self.value_net(x)\n",
    "\n",
    "        return {\n",
    "            \"master_policy\": master_policy,\n",
    "            \"q_option\": q_option,\n",
    "        }\n",
    "\n",
    "\n",
    "class LowerNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(LowerNetwork, self).__init__()\n",
    "\n",
    "        self.policy_net = FCNetwork(obs_dim, action_dim, nn.Tanh)\n",
    "        self.termination_net = FCNetwork(obs_dim, 1, nn.Sigmoid)\n",
    "        self.std = nn.Parameter(torch.zeros((1, action_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_action = self.policy_net(x)\n",
    "        std_action = F.softplus(self.std).expand(mean_action.size(0), -1) # ?\n",
    "        termination_prob = self.termination_net(x)\n",
    "\n",
    "        return {\n",
    "            \"mean_action\": mean_action,\n",
    "            \"std_action\": std_action,\n",
    "            \"termination_prob\": termination_prob,\n",
    "        }\n",
    "\n",
    "\n",
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim, output_dim, output_activation=None,\n",
    "        hidden_dims=(DEFAULT_HIDDEN_UNITS, DEFAULT_HIDDEN_UNITS), hidden_activation=nn.Tanh\n",
    "    ):\n",
    "        super(FCNetwork, self).__init__()\n",
    "\n",
    "        layers = list()\n",
    "        dims = (input_dim,) + hidden_dims\n",
    "        for in_dim, out_dim in zip(dims[:-1], dims[1:]):\n",
    "            layers.append(nn.Linear(in_features=in_dim, out_features=out_dim))\n",
    "            layers.append(hidden_activation())\n",
    "        layers.append(nn.Linear(in_features=hidden_dims[-1], out_features=output_dim))\n",
    "        if output_activation is not None:\n",
    "            layers.append(output_activation())\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        data = tensor(x)\n",
    "        for layer in self.layers:\n",
    "            data = layer(data)\n",
    "\n",
    "        return data\n",
    "    \n",
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step +\n",
    "                                               1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns\n",
    "\n",
    "\n",
    "def ppo_iter(mini_batch_size, states, actions, log_probs, rets, advs, beta_advs, qos, betas, entropies, options, prev_options, is_init_states):\n",
    "    batch_size = states.size(0)\n",
    "    idlist = np.random.permutation(batch_size)\n",
    "    for i in range(batch_size // mini_batch_size):\n",
    "#        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        rand_ids = idlist[i*mini_batch_size:min((i+1)*mini_batch_size, batch_size)]\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[\n",
    "            rand_ids, :], rets[rand_ids, :], advs[rand_ids, :], beta_advs[rand_ids, :], qos[rand_ids, :], betas[rand_ids, :], entropies[rand_ids, :], options[rand_ids, :], prev_options[rand_ids, :], is_init_states[rand_ids, :]\n",
    "\n",
    "# states.shape is [num_envs*mini_batch_size, observation_space]\n",
    "# dist.log_prob(action) gives a tensor with shape [num_envs*mini_batch_size, action_space],\n",
    "# thus needs to use .sum(1).unsqueeze(1) to transform to [num_envs*mini_batch_size, 1]\n",
    "def ppo_update(model,\n",
    "               optimizer,\n",
    "               ppo_epochs,\n",
    "               mini_batch_size,\n",
    "               states,\n",
    "               actions,\n",
    "               log_probs,\n",
    "rets, advs, beta_advs, qos, betas, entropies, options, prev_options, is_init_states,\n",
    "               clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, ret, adv, beta_adv, qo, beta, entropy, option, prev_option, is_init_state in ppo_iter(\n",
    "                mini_batch_size, states, actions, log_probs, rets, advs, beta_advs, qos, betas, entropies, options, prev_options, is_init_states):\n",
    "            prediction = model(state)\n",
    "            option_ = option.unsqueeze(-1).expand(-1, -1, prediction['mean'].size(-1))\n",
    "            mean = prediction['mean'].gather(1, option_).squeeze(1)\n",
    "            std = prediction['std'].gather(1, option_).squeeze(1)\n",
    "            dist = torch.distributions.Normal(mean, std)\n",
    "            #            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action).sum(-1).unsqueeze(-1)\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * adv\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param,\n",
    "                                1.0 + clip_param) * adv\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (ret - prediction['q_option'].gather(1, option)).pow(2).mean()\n",
    "            beta_loss = (beta.gather(1, prev_option) * beta_adv * (1 - is_init_state)).mean()\n",
    "            master_loss = -(prediction['master_policy'].gather(1, option) * adv).mean() - 0.01 * (-(prediction['master_policy']*prediction['master_policy'].log()).sum(-1).mean())#entropy\n",
    "            loss = 0.5 * critic_loss + actor_loss  + beta_loss + master_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "class oc_agent():\n",
    "    def __init__(self,\n",
    "                 num_envs=16,\n",
    "                 env_name=\"HalfCheetah-v2\",\n",
    "                 lr=3e-4,\n",
    "                 num_steps=2048,\n",
    "                 num_options=4,\n",
    "                 ppo_epochs=10,\n",
    "                 mini_batch_size=64,\n",
    "                 log_dir=\"data/\"):\n",
    "        self.num_envs = num_envs\n",
    "        self.env_name = env_name\n",
    "        self.lr = lr\n",
    "        self.num_steps = num_steps\n",
    "        self.envs = make_vec_env(make_env(env_name, log_dir), n_envs=self.num_envs)\n",
    "        self.envs = VecNormalize(self.envs, norm_reward=False)\n",
    "        self.num_inputs = self.envs.observation_space.shape[0]\n",
    "        self.num_outputs = self.envs.action_space.shape[0]\n",
    "        self.num_options = num_options\n",
    "        self.model = DACNetwork(self.num_inputs, self.num_outputs, self.num_options)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),\n",
    "                                    lr=self.lr,\n",
    "                                    eps=1e-5)\n",
    "        self.max_frames = 2000000\n",
    "        self.is_init_state = tensor(np.ones((self.num_envs))).byte()\n",
    "        self.worker_index = tensor(np.arange(self.num_envs)).long()\n",
    "        self.prev_option = tensor(np.zeros(self.num_envs)).long()\n",
    "        self.eps = 0.1\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "\n",
    "    def compute_pi_h(self, prediction, pre_option, is_init_states):\n",
    "        master_policy = prediction[\"master_policy\"]\n",
    "        beta = prediction[\"beta\"]\n",
    "\n",
    "        mask = torch.zeros_like(master_policy)\n",
    "        mask[self.worker_index, pre_option] = 1\n",
    "\n",
    "        # pi_h = beta * master_policy + (1 - beta) * mask\n",
    "        is_init_states = is_init_states.view(-1, 1).expand(-1, master_policy.size(1))\n",
    "        pi_h = torch.where(is_init_states, master_policy, beta * master_policy + (1 - beta) * mask)\n",
    "        # print(\"pi_h %s\" % pi_h)\n",
    "\n",
    "        return pi_h\n",
    "\n",
    "    def compute_pi_l(self, options, action, mean, std):\n",
    "        options = options.unsqueeze(-1).expand(-1, -1, mean.size(-1))\n",
    "        mean = mean.gather(1, options).squeeze(1)\n",
    "        std = std.gather(1, options).squeeze(1)\n",
    "        normal_dis = torch.distributions.Normal(mean, std)\n",
    "\n",
    "        pi_l = normal_dis.log_prob(action).sum(-1).exp().unsqueeze(-1)\n",
    "\n",
    "        return pi_l\n",
    "\n",
    "    def run(self):\n",
    "        frame_idx = 0\n",
    "        test_rewards = []\n",
    "        state = self.envs.reset()\n",
    "        cumu_rewd = np.zeros(self.num_envs)\n",
    "        path='./data/{}'.format(self.env_name)\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except OSError as error:\n",
    "            print(error)\n",
    "        curtime = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") \\\n",
    "                    + \"_{:04d}\".format(random.randint(1,9999))\n",
    "        fd_train = open(path + '/ppoc_train_{}.log'.format(curtime), 'w')\n",
    "        while frame_idx < self.max_frames:\n",
    "            log_probs = []\n",
    "\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            masks = []\n",
    "            np_masks = []\n",
    "            entropies = []\n",
    "            values = []\n",
    "            #            entropy = 0\n",
    "            prev_options = []\n",
    "            rets = []\n",
    "            advs = []\n",
    "            predictions = []\n",
    "            beta_advs = []\n",
    "            qos = []\n",
    "            options = []\n",
    "            betas = []\n",
    "            is_init_states = []\n",
    "            for _ in range(self.num_steps):\n",
    "                state = torch.FloatTensor(state)\n",
    "                prediction = self.model(state)\n",
    "                pi_h = self.compute_pi_h(prediction, self.prev_option, self.is_init_state)\n",
    "                option = torch.distributions.Categorical(probs = pi_h).sample()\n",
    "#                option = self.sample_option(prediction, self.eps, self.prev_option, self.is_init_state)\n",
    "                mean = prediction['mean'][self.worker_index, option]\n",
    "                std = prediction['std'][self.worker_index, option]\n",
    "                dist = torch.distributions.Normal(mean, std)\n",
    "                action = dist.sample()\n",
    "\n",
    "                log_prob = dist.log_prob(action).sum(-1).unsqueeze(-1)\n",
    "                entropy = dist.entropy().sum(-1).unsqueeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "                next_state, reward, done, info = self.envs.step(action.cpu().detach().numpy())\n",
    "                #done = done or truncated\n",
    "                cumu_rewd += reward\n",
    "                for i in range(self.num_envs):\n",
    "                    if done[i]:\n",
    "                        print(\"Cumulative reward at step \" + str(frame_idx) +\n",
    "                              \" is \" + str(cumu_rewd[i]))\n",
    "                        fd_train.write(\"%d %f\\n\" % (frame_idx, cumu_rewd[i]))\n",
    "                        cumu_rewd[i] = 0\n",
    "\n",
    "                    fd_train.flush()\n",
    "\n",
    "                log_probs.append(log_prob)\n",
    "                prev_options.append(self.prev_option.unsqueeze(-1))\n",
    "                options.append(option.unsqueeze(-1))\n",
    "                rewards.append(torch.FloatTensor(reward).unsqueeze(-1))\n",
    "                masks.append(torch.FloatTensor(1 - done).unsqueeze(-1))\n",
    "                entropies.append(entropy)\n",
    "                states.append(state)\n",
    "                values.append(prediction['q_option'][self.worker_index, option].unsqueeze(-1))\n",
    "                actions.append(action)\n",
    "                betas.append(prediction['beta'])\n",
    "                qos.append(prediction['q_option'])\n",
    "                is_init_states.append(self.is_init_state.unsqueeze(-1).float())\n",
    "                self.is_init_state = tensor(done).byte()\n",
    "                self.prev_option = option\n",
    "                state = next_state\n",
    "                frame_idx += self.num_envs\n",
    "\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            with torch.no_grad():\n",
    "                prediction = self.model(next_state)\n",
    "                betass = prediction['beta'][self.worker_index, self.prev_option]\n",
    "                ret = (1 - betass) * prediction['q_option'][self.worker_index, self.prev_option] + \\\n",
    "                  betass * torch.max(prediction['q_option'], dim=-1)[0]\n",
    "                ret = ret.unsqueeze(-1)\n",
    "\n",
    "            for i in reversed(range(self.num_steps)):\n",
    "#                v = qos[i].max(dim=-1, keepdim=True)[0] * (1 - self.eps) + qos[i].mean(-1).unsqueeze(-1) * self.eps\n",
    "                v = (prediction['q_option'] * prediction['master_policy']).sum(-1).unsqueeze(-1)\n",
    "                q = qos[i].gather(1, prev_options[i])\n",
    "                beta_advs.append(q - v + 0.01)\n",
    "            rets = compute_gae(ret, rewards, masks, values)\n",
    "\n",
    "            log_probs = torch.cat(log_probs).detach()\n",
    "            betas = torch.cat(betas).detach()\n",
    "            qos = torch.cat(qos).detach()\n",
    "            states = torch.cat(states).detach()\n",
    "            actions = torch.cat(actions).detach()\n",
    "            rets = torch.cat(rets).detach()\n",
    "            values = torch.cat(values).detach()\n",
    "            advs = rets - values\n",
    "            advs = (advs-advs.mean())/advs.std()\n",
    "            beta_advs = torch.cat(beta_advs).detach()\n",
    "            entropies = torch.cat(entropies).detach()\n",
    "            options = torch.cat(options).detach()\n",
    "            prev_options = torch.cat(prev_options).detach()\n",
    "            is_init_states = torch.cat(is_init_states).detach()\n",
    "            fd_train.close()\n",
    "            return states, actions, log_probs, rets, advs, beta_advs, qos, betas, entropies, options, prev_options, is_init_states \n",
    "            #ppo_update(self.model, self.optimizer, self.ppo_epochs,\n",
    "            #           self.mini_batch_size, states, actions, log_probs,\n",
    "            #           rets, advs, beta_advs, qos, betas, entropies, options, prev_options, is_init_states)\n",
    "        fd_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chpre\\projects\\thesis\\venv\\lib\\site-packages\\gymnasium\\core.py:297: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "ocagent = oc_agent(num_envs=2, env_name=\"QuadX-Hover-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A                             \u001b[A\n",
      "\u001b[A                             \u001b[A\n",
      "[WinError 183] Cannot create a file when that file already exists: './data/QuadX-Hover-v1'                             \u001b[A\n",
      "Cumulative reward at step 78 is -61.14630722999573\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 96 is -68.34467497840524                             \u001b[A\n",
      "Cumulative reward at step 118 is -75.07660397142172\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 146 is -73.78199069574475                             \u001b[A\n",
      "Cumulative reward at step 226 is -97.60599767975509\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 234 is -133.1422159075737                             \u001b[A\n",
      "Cumulative reward at step 274 is -72.59511531889439\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 302 is -73.78805456263945                             \u001b[A\n",
      "Cumulative reward at step 410 is -49.84902361035347\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 426 is -128.67704783566296                             \u001b[A\n",
      "Cumulative reward at step 518 is -124.03354455530643\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 554 is -77.47799838706851\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 574 is -169.50824576616287\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 622 is -70.68453757464886\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 634 is -58.423801600933075                             \u001b[A\n",
      "Cumulative reward at step 698 is -90.36223410815\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 714 is -92.17150948569179                             \u001b[A\n",
      "Cumulative reward at step 808 is -50.032832361757755\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 840 is -71.89669984579086                             \u001b[A\n",
      "Cumulative reward at step 860 is -79.21699881181121\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 880 is -73.51542164385319                             \u001b[A\n",
      "Cumulative reward at step 974 is -122.3579693492502\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 978 is -63.124430200085044                             \u001b[A\n",
      "Cumulative reward at step 1040 is -92.64630023390055\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 1066 is -73.31028448045254                             \u001b[A\n",
      "Cumulative reward at step 1144 is -46.92923580855131\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 1164 is -76.05449411273003                             \u001b[A\n",
      "Cumulative reward at step 1284 is -128.2800051588565\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 1294 is -191.92140469513834                             \u001b[A\n",
      "Cumulative reward at step 1376 is -151.17511823773384\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 1378 is -102.28594969213009                             \u001b[A\n",
      "Cumulative reward at step 1422 is -72.88920937106013\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 1490 is -122.74759991466999\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 1496 is -96.16732351481915                             \u001b[A\n",
      "Cumulative reward at step 1578 is -82.28753969445825\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 1594 is -108.33329537883401                             \u001b[A\n",
      "Cumulative reward at step 1648 is -107.17941057682037\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 1688 is -75.949531737715                             \u001b[A\n",
      "Cumulative reward at step 1802 is -116.64207373186946\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 1818 is -135.0177024276927                             \u001b[A\n",
      "Cumulative reward at step 1936 is -52.34757055807859\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 1948 is -177.84691487252712                             \u001b[A\n",
      "Cumulative reward at step 1978 is -76.720110706985                             \u001b[A\n",
      "Cumulative reward at step 2024 is -67.86664476245642\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2068 is -69.70763760618865\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2122 is -82.5172027888766\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2170 is -69.83463145047426                             \u001b[A\n",
      "Cumulative reward at step 2218 is -118.41508669406176\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2226 is -97.67685391381383                             \u001b[A\n",
      "Cumulative reward at step 2326 is -58.820380790159106\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2382 is -137.00922872498631\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2454 is -84.83127619698644\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2514 is -44.13997662626207\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2546 is -144.70287097245455                             \u001b[A\n",
      "Cumulative reward at step 2658 is -109.49636340793222\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2668 is -65.48713905736804                             \u001b[A\n",
      "Cumulative reward at step 2728 is -77.45420455932617\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2742 is -81.34683665074408                             \u001b[A\n",
      "Cumulative reward at step 2794 is -68.09997968189418\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2832 is -76.071932323277\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2894 is -71.4927963167429\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2912 is -97.90420015901327\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 2982 is -77.60777432844043\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 3012 is -73.59951177239418                             \u001b[A\n",
      "Cumulative reward at step 3056 is -52.754575714468956                             \u001b[A\n",
      "Cumulative reward at step 3126 is -73.53864750266075\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 3132 is -105.04159198701382                             \u001b[A\n",
      "Cumulative reward at step 3212 is -55.80653081834316\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 3228 is -42.8842411339283                             \u001b[A\n",
      "Cumulative reward at step 3264 is -69.71828072052449                             \u001b[A\n",
      "Cumulative reward at step 3310 is -73.60646767914295\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 3330 is -68.80524289701134                             \u001b[A\n",
      "Cumulative reward at step 3438 is -66.33383191935718\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 3476 is -151.92273490829393\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 3510 is -65.53784596920013\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 3540 is -91.54591201245785                             \u001b[A\n",
      "Cumulative reward at step 3584 is -74.06546491384506                             \u001b[A\n",
      "Cumulative reward at step 3666 is -111.30489876121283                             \u001b[A\n",
      "Cumulative reward at step 3720 is -120.78103556856513\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 3752 is -70.12349580228329                             \u001b[A\n",
      "Cumulative reward at step 3828 is -109.14602510258555\n",
      "\u001b[A                             \u001b[A\n",
      "Cumulative reward at step 3856 is -96.80568246170878                             \u001b[A\n",
      "Cumulative reward at step 3914 is -78.62986869364977                             \u001b[A\n",
      "Cumulative reward at step 3948 is -84.47397803887725                             \u001b[A\n",
      "Cumulative reward at step 4002 is -64.95667975395918                             \u001b[A\n",
      "Cumulative reward at step 4062 is -74.70453523099422\n"
     ]
    }
   ],
   "source": [
    "states, actions, log_probs, rets, advs, beta_advs, qos, betas, entropies, options, prev_options, is_init_states  = ocagent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = ocagent.model(states)\n",
    "option_ = options.unsqueeze(-1).expand(-1, -1, prediction['mean'].size(-1))\n",
    "mean = prediction['mean'].gather(1, option_).squeeze(1)\n",
    "std = prediction['std'].gather(1, option_).squeeze(1)\n",
    "dist = torch.distributions.Normal(mean, std)\n",
    "#            entropy = dist.entropy().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.5000],\n",
       "        [0.6175],\n",
       "        ...,\n",
       "        [0.4256],\n",
       "        [0.4969],\n",
       "        [0.7296]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas.gather(1, prev_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "layers = nn.Sequential(\n",
    "    nn.Linear(21, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 4)\n",
    ")\n",
    "\n",
    "layers2 = nn.Sequential(\n",
    "    nn.Linear(21, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 4),\n",
    "    nn.Softmax(dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1409, -0.0223,  0.0165,  0.1972], grad_fn=<ViewBackward0>)\n",
      "tensor([0.2638, 0.2241, 0.2330, 0.2791], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([-1.3325, -1.4956, -1.4569, -1.2762], grad_fn=<LogBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = layers(states[0])\n",
    "print(x)\n",
    "soft = nn.Softmax(dim=-1)\n",
    "print(soft(x))\n",
    "print(soft(x).log())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'master_policy': tensor([0.2499, 0.2499, 0.2502, 0.2499], grad_fn=<SoftmaxBackward0>),\n",
       " 'q_option': tensor([ 0.0003, -0.0003,  0.0001, -0.0011], grad_fn=<ViewBackward0>)}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocagent.model.higher_net(states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.4999, 0.5000, 0.5000])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.7882, -0.6587, -1.0759, -0.5713], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_log_probs[range(actions.squeeze.shape[0]), actions.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.7882, -0.6587, -1.0759, -0.5713], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_log_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6587, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_log_probs[range(options.squeeze().shape[0]), options.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0174, -1.1319, -1.8766, -0.8604], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(dist.cdf(actions)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log_probs = dist.log_prob(action).sum(-1).unsqueeze(-1)\n",
    "ratio = (new_log_probs - old_log_probs).exp()\n",
    "surr1 = ratio * adv\n",
    "surr2 = torch.clamp(ratio, 1.0 - clip_param,\n",
    "                    1.0 + clip_param) * adv\n",
    "\n",
    "actor_loss = -torch.min(surr1, surr2).mean()\n",
    "critic_loss = (ret - prediction['q_option'].gather(1, option)).pow(2).mean()\n",
    "beta_loss = (beta.gather(1, prev_option) * beta_adv * (1 - is_init_state)).mean()\n",
    "master_loss = -(prediction['master_policy'].gather(1, option) * adv).mean() - 0.01 * (-(prediction['master_policy']*prediction['master_policy'].log()).sum(-1).mean())#entropy\n",
    "loss = 0.5 * critic_loss + actor_loss  + beta_loss + master_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
